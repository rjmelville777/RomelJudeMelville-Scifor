# -*- coding: utf-8 -*-
"""Week 2 Assignment.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RDNim7uuLdu_N5ntxtMz0OMCs7mdUsC8
"""

1.What is the difference between a list and a tuple in Python?

Lists are mutable,meaning we can add or remove elements after creating a list. Methods like append(), extend(), insert(), and pop() can be used to modify a list.
Tuples are immutable,meaning once we create a tuple, it cannot be modified.

Lists are defined using square brackets []
Eg: my_list = [1, 2, 3]
Tuples are defined using parentheses ()
Eg: my_tuple = (1, 2, 3)

Lists does not have much efficiency in their working while Tuple is more efficient in their working when compared to lists

List is a slow process while Tuple is a fast process

2.How can you iterate through a list in Python

You can iterate through a list in Python using various methods :
1. Using For loop
2. Using while loop
3. Using list comprehension
4. Using lambda and map functions

3.How do you handle exceptions in Python?

The try and except block in Python is used to catch and handle exceptions.The statement that can raise the exceptions are kept inside the try clause and the
statements that handle the exceptions are written inside except clause.
The syntax is:
try:
    # Code that might raise an exception
except Exception as e:
    # Handle the exception
    print(e)

4.What are list comprehensions in Python?

List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list

divisible = [x for x in range(1,6) if x % 2 == 0]
print(divisible)

5.What is the purpose of the if __name__ == "__main__" statement?

The "if name == 'main'" statement in Python is used to control the execution of code within a script. It checks whether the script is being run as the main program or is being imported as a module into another script.

6.What is the purpose of the with statement in Python?

The with statement is used in exception handling to make the code cleaner and much more readable.The with statement helps avoiding bugs and leaks by ensuring that a resource is properly released when the code using the resource is completely executed.

7.What are the key features of Spark?

Apache Spark is a big data platform that enables efficient processing of large-scale data sets. It has several key features, including:

Integration: Easy integration with Hadoop and HDFS files
Languages: Supports writing applications in Java, Scala, Python, R, and SQL
Speed: Apache Spark is designed for speed and can process large datasets quickly.
RDDs: Uses Resilient Distributed Datasets which are an immutable distributed collection of objects

8.What are Resilient Distributed Datasets (RDDs) in Spark?

Resilient Distributed Datasets (RDDs) are the primary data structure in Spark. RDDs are reliable and memory-efficient when it comes to parallel processing. By storing and processing data in RDDs, Spark speeds up MapReduce processes
They are fault-tolerant and can be operated on many devices at the same time.

9.What is the difference between a DataFrame and an RDD in Spark?

1. RDDs are less structured and closer to Scala collections or lists. DataFrames are organized into named columns
2. RDDs are the most basic and low-level API while Dataframes are high API
3. RDDs are not optimized for SQL while Dataframes are optimized for SQL

10.What is Spark's ecosystem?

The Spark ecosystem consists of various components including:
1. Spark SQL: Spark SQL is a module for structured data processing that provides a programming interface for working with structured and semi-structured data
2. MLlib (Machine Learning Library): MLlib is a machine learning library for Spark that provides a wide range of machine learning algorithms and tools for building and deploying machine learning models at scale.
3. GraphX: GraphX is a graph processing library for Spark that provides an API for working with graph data structures and performing graph computations
4. Spark Streaming: Spark Streaming is a real-time data processing module that allows you to process and analyze data streams in near real-time
5. SparkR: SparkR is an R package that enables R users to work with Spark
6. PySpark: PySpark is the Python API for Spark